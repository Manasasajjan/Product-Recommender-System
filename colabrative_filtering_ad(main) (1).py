# -*- coding: utf-8 -*-
"""Colabrative_Filtering_AD(Main).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OJFkXrl60-R_VO5-8tB2ednyvT-zKSNy

#Memory /Model Based Colabrative Filtering

**Exploratory Data Analysis**

We first import the relevant libraries.
"""

import pandas as pd
import numpy as np
import tensorflow as tf

"""Reading the dataset with appropriate columns and printing its first few columns"""

columns = ['productID', 'userID', 'ratings','timestamp']
recomm_df1 = pd.read_csv('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv',names=columns)
# https://jmcauley.ucsd.edu/data/amazon_v2/categoryFiles/Electronics.json.gz
# http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz
# http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv

recomm_df1.info()

"""Removing 'timestamp' column as it is not relevant to us"""

df1 = recomm_df1.head(20000)

df1.head(10)

df = df1.drop('timestamp', axis=1)

"""The following command gives details about the dataset."""

df.info()

from tensorflow.keras.preprocessing.text import Tokenizer

lst1 = list(df["productID"].values)
tokenizer1 = Tokenizer()
tokenizer1.fit_on_texts(lst1)
sequences = tokenizer1.texts_to_sequences(lst1)

word_index_1 = tokenizer1.word_index

tokenizer1.word_index

word_index_1

lst = list(df['userID'].values)

tokenizer = Tokenizer()

tokenizer.fit_on_texts(lst)
sequences = tokenizer.texts_to_sequences(lst)

tokenizer.word_index

len(tokenizer.word_index)
word_index =  tokenizer.word_index

userID_num = []
productID_num = []
user_IDs = list(df['userID'].values)
product_IDs = list(df['productID'].values)
for i in user_IDs:
    userID_num.append(word_index[i.lower()])
for i in product_IDs:
    productID_num.append(word_index_1[i.lower()])
df['userID_num'] = userID_num
df['productID_num'] = productID_num

df.head(10)

df.info()

"""Checking the various mathemaical functions about 'ratings' column, we find that most of the reviews are 5.0."""

df["ratings"].describe()

"""Gives the number of empty cells in each column. Since all are 0, it implies we dont have unknown values in our dataset."""

df.isna().sum()

"""We find that there are 20994353 rows and 3 columns in our dataset."""

df.shape

"""We look for different users and the number of ratings that they have given."""

df.userID.value_counts()

"""The following commands give their respective values."""

print('Number of unique users', len(df['userID'].unique()))
print('Number of unique products', len(df['productID'].unique()))
print('Unique Ratings', df['ratings'].unique())

n_users=int(df.userID_num.nunique())
n_items=int(df.productID_num.nunique())

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(df, test_size=0.25)

train_data_matrix = np.zeros((n_users, n_items))
for line in train_data.itertuples():
    train_data_matrix[line[4]-1, line[5]-1] = line[3]

test_data_matrix = np.zeros((n_users, n_items))
for line in test_data.itertuples():
    test_data_matrix[line[4]-1, line[5]-1] = line[3]

"""#MEMORY BASED"""

def predict(ratings, similarity, type='user'):
    if type == 'user':
        mean_user_rating = ratings.mean(axis=1)
        #You use np.newaxis so that mean_user_rating has same format as ratings
        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])
        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T
    elif type == 'item':
        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])
    return pred

from sklearn.metrics.pairwise import pairwise_distances
user_similarity = pairwise_distances(train_data_matrix, metric='cosine')
item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')

item_prediction = predict(train_data_matrix, item_similarity, type='item')
user_prediction = predict(train_data_matrix, user_similarity, type='user')

from sklearn.metrics import mean_squared_error
from math import sqrt
def mse(prediction, ground_truth):
    prediction = prediction[ground_truth.nonzero()].flatten()
    ground_truth = ground_truth[ground_truth.nonzero()].flatten()
    return mean_squared_error(prediction, ground_truth)

from sklearn.metrics import mean_squared_error
from math import sqrt

def mse(prediction, ground_truth):
    prediction = prediction[ground_truth.nonzero()].flatten()
    ground_truth = ground_truth[ground_truth.nonzero()].flatten()
    return mean_squared_error(prediction, ground_truth)

def rmse(prediction, ground_truth):
    mse_value = mse(prediction, ground_truth)
    rmse_value = sqrt(mse_value)
    return rmse_value

from sklearn.metrics import mean_squared_error
from math import sqrt

def rmse(prediction, ground_truth):
    prediction = prediction[ground_truth.nonzero()].flatten()
    ground_truth = ground_truth[ground_truth.nonzero()].flatten()

    mse_value = mean_squared_error(prediction, ground_truth)
    rmse_value = sqrt(mse_value)

    return rmse_value

print('User-based CF MSE: ' , str(mse(user_prediction, test_data_matrix)))
print('Item-based CF MSE: ' , str(mse(item_prediction, test_data_matrix)))
print('User-based CF RMSE: ' , str(rmse(user_prediction, test_data_matrix)))
print('Item-based CF RMSE: ' , str(rmse(item_prediction, test_data_matrix)))

"""**Data Preprocessing**

We take data having only those users that have given more than 50 ratings and products that have more than 50 ratings.

First we find users who have made more than 50 reviews and create a new table having these values.
"""

userID = recomm_df1.groupby('userID').count()
top_user = userID[userID['ratings'] >= 50].index
topuser_ratings_df = recomm_df1[recomm_df1['userID'].isin(top_user)]
topuser_ratings_df.shape

topuser_ratings_df.head()

"""The data has been reduced to 394059 rows. We now arrange the ratings in decreasing order."""

topuser_ratings_df.sort_values(by='ratings', ascending=False).head()

"""We now do the same for ratings."""

prodID = recomm_df1.groupby('productID').count()

top_prod = prodID[prodID['ratings'] >= 50].index
top_ratings_df = topuser_ratings_df[topuser_ratings_df['productID'].isin(top_prod)]
top_ratings_df.sort_values(by='ratings', ascending=False).head()

top_ratings_df.shape

"""The number of entries have now been further reduced to 291192.

**Building the Collaborative Filtering Model**

We convert the data that we have now to a matrix.
"""

user_ratings = top_ratings_df.pivot_table(index=['userID'], columns=['productID'], values='ratings')
user_ratings.head()

"""The NaN are values for which a rating does not exist. We fill them with 0."""

user_ratings=user_ratings.dropna(thresh=50, axis=1).fillna(0)

user_ratings.head()

"""We now find the Pearson correlation of each product with respect to each other to know the realtions that exist between them - item to item filtering."""

item_similarity_df=user_ratings.corr(method='pearson')
item_similarity_df.head(10)

"""Now all we have to do is to create a function that finds the best products to recommend and arranges it in decreasing order. Then we pass an example of a user to it with products and the ratings the user has given to the function for each product and rating.  """

df.head()

def get_similar_products(product_name, user_rating):
  similar_score=item_similarity_df[product_name]*(user_rating-2.5)
  similar_score=similar_score.sort_values(ascending=False)
  return similar_score

print(get_similar_products("A34PAZQ73SL163",5))

def get_products_rated_by_user(user_id, df):
    user_ratings = df[df['userID'] == user_id]
    rated_products = user_ratings['productID'].tolist()
    return rated_products

# Usage example
rated_products = get_products_rated_by_user("0307142485" , top_ratings_df)

print(rated_products)

row = top_ratings_df.loc[(top_ratings_df['userID'] == "0307142485")]

row

example=[("A34PAZQ73SL163", 5),("A3CWH6VKCTJAD", 3),("A3E102F6LPUF1J", 5)]
similar_products=pd.DataFrame()
for product, rating in example:
  similar_products=similar_products.append(get_similar_products(product, rating),ignore_index=True)
similar_products.head()
similar_products.sum().sort_values(ascending=False)

"""We can see that the model is successful because the user's best recommendation is the one that he had rated 5.

#Model Based CF
"""

columns = ['productID', 'userID', 'ratings','timestamp']
recomm_df = pd.read_csv('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv',names=columns)

recomm_df= recomm_df.head(154482)
recomm_df.info()

from tensorflow.keras.preprocessing.text import Tokenizer

lst = list(recomm_df['userID'].values)

tokenizer = Tokenizer()

tokenizer.fit_on_texts(lst)
sequences = tokenizer.texts_to_sequences(lst)

tokenizer.word_index

len(tokenizer.word_index)

recomm_df = recomm_df.drop('timestamp', axis=1)

recomm_df.head(20)

recomm_df.info()

recomm_df.describe()

recomm_df.shape

recomm_df.userID.value_counts()

print('Number of unique users', len(recomm_df['userID'].unique()))
print('Number of unique products', len(recomm_df['productID'].unique()))
print('Unique Ratings', recomm_df['ratings'].unique())

pip install scikit-surprise

from surprise import Reader, Dataset

reader = Reader(rating_scale=(0.5, 5.0))

reader

ratings_dict = {'productID': list(recomm_df.productID),
                'userID': list(recomm_df.userID),
                'ratings': list(recomm_df.ratings)}
df = pd.DataFrame(ratings_dict)

data = Dataset.load_from_df(df[['userID', 'productID', 'ratings']], reader)

from surprise.model_selection import KFold
kf = KFold(n_splits=6)
kf.split(data)

from surprise import SVD
from surprise.model_selection import cross_validate
algo = SVD()
# evaluate(algo, data, measures=['RMSE'])
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=6, verbose=True)

# from surprise import SVD
# from surprise.model_selection import GridSearchCV
# param_grid = {'n_factors': [50, 100, 150],
#               'n_epochs': [20, 30, 40],
#               'lr_all': [0.005, 0.01, 0.02],
#               'reg_all': [0.02, 0.04, 0.06]}
# algo = SVD()

# gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)
# gs.fit(data)



# best_rmse = gs.best_score['rmse']
# best_params = gs.best_params['rmse']

# print(f"Best RMSE: {best_rmse}")
# print(f"Best Parameters: {best_params}")

from surprise import NMF
algo = NMF()
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

from surprise import KNNBasic
algo = KNNBasic()
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

import matplotlib.pyplot as plt

mses = [6.47, .957, .897, .804, .801]
algos = ['cosine_memory', 'KNN', "NMF", 'SVD', 'PMF']
plt.plot(algos, mses, 'go',  )
plt.xlabel("Different algos")
plt.ylabel("MSE")
plt.show()

import pandas as pd
from surprise import SVD
from surprise.model_selection import train_test_split
from surprise import Dataset, Reader

# Example DataFrame
ratings_dict = {'productID': list(recomm_df.productID),
                'userID': list(recomm_df.userID),
                'ratings': list(recomm_df.ratings)}
data = pd.DataFrame(ratings_dict)

def predict_similar_products(product_id, user_id, user_rating, data):
    # 'data' is the DataFrame containing user-item ratings

    # Step 1: Convert the DataFrame to a Surprise Dataset
    reader = Reader(rating_scale=(1, 5))  # Assuming ratings are on a scale of 1 to 5
    surprise_data = Dataset.load_from_df(data[['userID', 'productID', 'ratings']], reader)

    # Step 2: Train the SVD model on the entire dataset
    algo = SVD()
    trainset = surprise_data.build_full_trainset()
    algo.fit(trainset)

    # Step 3: Use the trained model to predict ratings for all other products for the given user
    testset = [(user_id, product_id, user_rating)]
    predicted_rating = algo.test(testset)[0].est

    # Step 4: Get similar products based on predicted ratings
    product_names = data['productID'].unique()
    similar_products = {}
    for prod_id in product_names:
        if prod_id != product_id:
            testset = [(user_id, prod_id, user_rating)]  # Use the same user_rating for all products
            predicted_rating = algo.test(testset)[0].est
            similar_products[prod_id] = predicted_rating

    # Sort the similar products based on predicted ratings (in descending order)
    similar_products = sorted(similar_products.items(), key=lambda x: x[1], reverse=True)

    return similar_products

# Example usage:
input_product_id = 'A2CPBQ5W4OGBX'
input_user_id = '0528881469'
input_user_rating = 2.0

similar_products = predict_similar_products(input_product_id, input_user_id, input_user_rating, data)
print(similar_products)